{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da5a156",
   "metadata": {},
   "source": [
    "### Deep Q-learning using keras-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf3425",
   "metadata": {},
   "source": [
    "In this lab we will change gears just a bit and start using some of the deep q-learning software that is generally available. While it is possible to write your own code, it does take quite a bit of time to do so. One needs to implement Q-learning as well as neural network code. (FWIW, I had thought to do this with scikit-learn, but it is still a lot of code to write.)\n",
    "\n",
    "For this lab we will follow the example at [A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python](https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/)\n",
    "\n",
    "I encourage you to read through the whole article. However, for the computer lab experiment go to the section, __Implementing Deep Q-Learning in Python using Keras & OpenAI Gym__.\n",
    "\n",
    "You should already have installed the OpenAI Gym environment using\n",
    "```text\n",
    "conda install -c conda-forge gym\n",
    "```\n",
    "You can use __conda__ to install keras similarly\n",
    "```text\n",
    "conda install -c conda-forge keras\n",
    "```\n",
    "\n",
    "Keras does not contain the reinforcement learning code. It must be installed seperately. The blog refers to __keras-rl__. However, it has subsequently been updated to __keras-rl2__. It cannot be installed using __conda__. Instead, you can either download it for the GitHub repository, or more simply install it using __pip__.\n",
    "```texrt\n",
    "pip install keras-rl2\n",
    "```\n",
    "\n",
    "Finally, remember that the environment visualizations cannot (easily) be run from the __jupyter__ environment. Open a terminal prompt and run python on __cartpole6.py__. You should see a training phase for the deep learning neural network(s) for the cartpole problem. This is followed by test the trained neural network.\n",
    "\n",
    "There a many things one might investigate for this lab. For instance, one might look into the keras documentation for creating other neural network designs. See for instance, [Keras Dense layer](https://keras.io/api/layers/core_layers/dense/). Also, you will observe that the deep learning neural network does not perform all that well. What might be done to improve its performance? \n",
    "\n",
    "You do not need to answer my questions. Feel free to perform your own investigations!\n",
    "\n",
    "--Doug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992b9ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   10/5000: episode: 1, duration: 0.051s, episode steps:  10, steps per second: 196, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22/5000: episode: 2, duration: 0.435s, episode steps:  12, steps per second:  28, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.468772, mae: 0.643310, mean_q: -0.288348\n",
      "   32/5000: episode: 3, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.402383, mae: 0.580940, mean_q: -0.195034\n",
      "   42/5000: episode: 4, duration: 0.065s, episode steps:  10, steps per second: 153, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.346710, mae: 0.525158, mean_q: -0.098503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54/5000: episode: 5, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.292350, mae: 0.452721, mean_q: 0.014419\n",
      "   63/5000: episode: 6, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.227877, mae: 0.391144, mean_q: 0.107997\n",
      "   73/5000: episode: 7, duration: 0.066s, episode steps:  10, steps per second: 152, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.201441, mae: 0.352446, mean_q: 0.199884\n",
      "   82/5000: episode: 8, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.165367, mae: 0.309966, mean_q: 0.297860\n",
      "   92/5000: episode: 9, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.136522, mae: 0.283105, mean_q: 0.400724\n",
      "  101/5000: episode: 10, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.120434, mae: 0.289858, mean_q: 0.511457\n",
      "  110/5000: episode: 11, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.105777, mae: 0.299601, mean_q: 0.625351\n",
      "  120/5000: episode: 12, duration: 0.063s, episode steps:  10, steps per second: 157, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.080733, mae: 0.288527, mean_q: 0.734952\n",
      "  130/5000: episode: 13, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.084450, mae: 0.322645, mean_q: 0.807749\n",
      "  140/5000: episode: 14, duration: 0.064s, episode steps:  10, steps per second: 157, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.076803, mae: 0.348370, mean_q: 0.895162\n",
      "  151/5000: episode: 15, duration: 0.070s, episode steps:  11, steps per second: 158, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.083551, mae: 0.382498, mean_q: 0.966494\n",
      "  161/5000: episode: 16, duration: 0.065s, episode steps:  10, steps per second: 153, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.076946, mae: 0.414884, mean_q: 1.067027\n",
      "  171/5000: episode: 17, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.073656, mae: 0.430834, mean_q: 1.149689\n",
      "  181/5000: episode: 18, duration: 0.065s, episode steps:  10, steps per second: 153, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.068198, mae: 0.428523, mean_q: 1.237362\n",
      "  191/5000: episode: 19, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.066309, mae: 0.461576, mean_q: 1.318154\n",
      "  200/5000: episode: 20, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.068907, mae: 0.512120, mean_q: 1.396587\n",
      "  208/5000: episode: 21, duration: 0.056s, episode steps:   8, steps per second: 142, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.106224, mae: 0.571136, mean_q: 1.386369\n",
      "  218/5000: episode: 22, duration: 0.065s, episode steps:  10, steps per second: 153, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.075767, mae: 0.569041, mean_q: 1.447632\n",
      "  228/5000: episode: 23, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.093235, mae: 0.617058, mean_q: 1.529340\n",
      "  238/5000: episode: 24, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.088757, mae: 0.638292, mean_q: 1.595650\n",
      "  248/5000: episode: 25, duration: 0.064s, episode steps:  10, steps per second: 156, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.095817, mae: 0.677414, mean_q: 1.622690\n",
      "  257/5000: episode: 26, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.091358, mae: 0.685991, mean_q: 1.692041\n",
      "  267/5000: episode: 27, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.087600, mae: 0.710229, mean_q: 1.777008\n",
      "  276/5000: episode: 28, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.100172, mae: 0.755919, mean_q: 1.866722\n",
      "  286/5000: episode: 29, duration: 0.063s, episode steps:  10, steps per second: 158, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.115645, mae: 0.807089, mean_q: 1.907436\n",
      "  295/5000: episode: 30, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.131522, mae: 0.868528, mean_q: 1.948608\n",
      "  305/5000: episode: 31, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.125674, mae: 0.875483, mean_q: 1.980081\n",
      "  314/5000: episode: 32, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.133242, mae: 0.929861, mean_q: 2.073599\n",
      "  323/5000: episode: 33, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.137117, mae: 0.928642, mean_q: 2.115031\n",
      "  335/5000: episode: 34, duration: 0.076s, episode steps:  12, steps per second: 157, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.123535, mae: 0.953274, mean_q: 2.198044\n",
      "  343/5000: episode: 35, duration: 0.056s, episode steps:   8, steps per second: 143, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.142350, mae: 0.997539, mean_q: 2.286642\n",
      "  353/5000: episode: 36, duration: 0.065s, episode steps:  10, steps per second: 155, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.118945, mae: 0.973231, mean_q: 2.370848\n",
      "  363/5000: episode: 37, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.131061, mae: 1.013520, mean_q: 2.463610\n",
      "  371/5000: episode: 38, duration: 0.052s, episode steps:   8, steps per second: 153, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.134638, mae: 1.069925, mean_q: 2.491218\n",
      "  381/5000: episode: 39, duration: 0.067s, episode steps:  10, steps per second: 150, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.143981, mae: 1.117874, mean_q: 2.535620\n",
      "  390/5000: episode: 40, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.092964, mae: 1.096544, mean_q: 2.636861\n",
      "  398/5000: episode: 41, duration: 0.053s, episode steps:   8, steps per second: 151, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.124270, mae: 1.129289, mean_q: 2.726541\n",
      "  409/5000: episode: 42, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.142878, mae: 1.158343, mean_q: 2.767787\n",
      "  417/5000: episode: 43, duration: 0.057s, episode steps:   8, steps per second: 139, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.185383, mae: 1.227288, mean_q: 2.768838\n",
      "  427/5000: episode: 44, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.201283, mae: 1.260740, mean_q: 2.756138\n",
      "  436/5000: episode: 45, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.180950, mae: 1.243441, mean_q: 2.827380\n",
      "  445/5000: episode: 46, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.171133, mae: 1.237461, mean_q: 2.943720\n",
      "  454/5000: episode: 47, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.233211, mae: 1.264577, mean_q: 3.006975\n",
      "  465/5000: episode: 48, duration: 0.078s, episode steps:  11, steps per second: 140, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.137244, mae: 1.181321, mean_q: 3.044852\n",
      "  475/5000: episode: 49, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.254896, mae: 1.286609, mean_q: 3.067067\n",
      "  485/5000: episode: 50, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.192218, mae: 1.262481, mean_q: 3.108034\n",
      "  495/5000: episode: 51, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.194856, mae: 1.277809, mean_q: 3.247276\n",
      "  505/5000: episode: 52, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.203813, mae: 1.322680, mean_q: 3.262533\n",
      "  515/5000: episode: 53, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.149589, mae: 1.308729, mean_q: 3.366515\n",
      "  526/5000: episode: 54, duration: 0.096s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.178446, mae: 1.336949, mean_q: 3.452072\n",
      "  536/5000: episode: 55, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.205373, mae: 1.394069, mean_q: 3.430626\n",
      "  546/5000: episode: 56, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.222361, mae: 1.413018, mean_q: 3.449094\n",
      "  555/5000: episode: 57, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.225249, mae: 1.426875, mean_q: 3.550642\n",
      "  565/5000: episode: 58, duration: 0.073s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.232594, mae: 1.456521, mean_q: 3.583741\n",
      "  573/5000: episode: 59, duration: 0.060s, episode steps:   8, steps per second: 134, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.131575, mae: 1.415912, mean_q: 3.647167\n",
      "  583/5000: episode: 60, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.163912, mae: 1.448162, mean_q: 3.745892\n",
      "  592/5000: episode: 61, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.222495, mae: 1.519786, mean_q: 3.822997\n",
      "  602/5000: episode: 62, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.270248, mae: 1.573207, mean_q: 3.777009\n",
      "  613/5000: episode: 63, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.233803, mae: 1.580262, mean_q: 3.775854\n",
      "  623/5000: episode: 64, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.248183, mae: 1.600872, mean_q: 3.914301\n",
      "  633/5000: episode: 65, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.248448, mae: 1.612450, mean_q: 3.937124\n",
      "  643/5000: episode: 66, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.193310, mae: 1.604155, mean_q: 4.043727\n",
      "  655/5000: episode: 67, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.221389, mae: 1.631138, mean_q: 4.028125\n",
      "  667/5000: episode: 68, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.278581, mae: 1.705103, mean_q: 4.082812\n",
      "  677/5000: episode: 69, duration: 0.089s, episode steps:  10, steps per second: 113, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.190320, mae: 1.674657, mean_q: 4.122365\n",
      "  687/5000: episode: 70, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.212617, mae: 1.709572, mean_q: 4.137663\n",
      "  696/5000: episode: 71, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.252597, mae: 1.760001, mean_q: 4.270949\n",
      "  704/5000: episode: 72, duration: 0.066s, episode steps:   8, steps per second: 121, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.235507, mae: 1.753348, mean_q: 4.183645\n",
      "  714/5000: episode: 73, duration: 0.068s, episode steps:  10, steps per second: 148, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.194914, mae: 1.740354, mean_q: 4.239726\n",
      "  722/5000: episode: 74, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.198893, mae: 1.774076, mean_q: 4.498041\n",
      "  732/5000: episode: 75, duration: 0.075s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.198337, mae: 1.790301, mean_q: 4.499096\n",
      "  741/5000: episode: 76, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.197753, mae: 1.811079, mean_q: 4.497111\n",
      "  756/5000: episode: 77, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.262799, mae: 1.882174, mean_q: 4.483501\n",
      "  764/5000: episode: 78, duration: 0.058s, episode steps:   8, steps per second: 137, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.245394, mae: 1.896836, mean_q: 4.394038\n",
      "  773/5000: episode: 79, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.241096, mae: 1.916483, mean_q: 4.587748\n",
      "  785/5000: episode: 80, duration: 0.085s, episode steps:  12, steps per second: 140, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.234690, mae: 1.985171, mean_q: 4.645396\n",
      "  794/5000: episode: 81, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.242102, mae: 1.980519, mean_q: 4.593192\n",
      "  804/5000: episode: 82, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.224197, mae: 2.023934, mean_q: 4.663133\n",
      "  815/5000: episode: 83, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.199276, mae: 2.031402, mean_q: 4.696238\n",
      "  824/5000: episode: 84, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.247332, mae: 2.080778, mean_q: 4.724287\n",
      "  833/5000: episode: 85, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.177973, mae: 2.065099, mean_q: 4.830894\n",
      "  842/5000: episode: 86, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.282325, mae: 2.114086, mean_q: 4.767694\n",
      "  854/5000: episode: 87, duration: 0.084s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.201882, mae: 2.128767, mean_q: 4.896544\n",
      "  864/5000: episode: 88, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.163732, mae: 2.135519, mean_q: 4.929884\n",
      "  874/5000: episode: 89, duration: 0.074s, episode steps:  10, steps per second: 134, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.169205, mae: 2.169920, mean_q: 5.001615\n",
      "  885/5000: episode: 90, duration: 0.102s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.186016, mae: 2.197423, mean_q: 5.059978\n",
      "  898/5000: episode: 91, duration: 0.108s, episode steps:  13, steps per second: 120, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.187693, mae: 2.228223, mean_q: 5.058012\n",
      "  908/5000: episode: 92, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.217558, mae: 2.296516, mean_q: 5.125184\n",
      "  918/5000: episode: 93, duration: 0.086s, episode steps:  10, steps per second: 116, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.143667, mae: 2.225529, mean_q: 4.981127\n",
      "  928/5000: episode: 94, duration: 0.071s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.164234, mae: 2.299164, mean_q: 5.187383\n",
      "  936/5000: episode: 95, duration: 0.060s, episode steps:   8, steps per second: 134, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.208735, mae: 2.259658, mean_q: 4.932574\n",
      "  947/5000: episode: 96, duration: 0.120s, episode steps:  11, steps per second:  92, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.190834, mae: 2.320154, mean_q: 5.086658\n",
      "  958/5000: episode: 97, duration: 0.092s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.207043, mae: 2.305237, mean_q: 4.927636\n",
      "  967/5000: episode: 98, duration: 0.073s, episode steps:   9, steps per second: 122, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.187401, mae: 2.372839, mean_q: 5.135552\n",
      "  976/5000: episode: 99, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.163717, mae: 2.428367, mean_q: 5.253786\n",
      "  986/5000: episode: 100, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.179899, mae: 2.414866, mean_q: 5.070322\n",
      "  998/5000: episode: 101, duration: 0.095s, episode steps:  12, steps per second: 126, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.185688, mae: 2.480521, mean_q: 5.240778\n",
      " 1009/5000: episode: 102, duration: 0.077s, episode steps:  11, steps per second: 143, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.179532, mae: 2.512775, mean_q: 5.251265\n",
      " 1020/5000: episode: 103, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.154243, mae: 2.543761, mean_q: 5.301213\n",
      " 1029/5000: episode: 104, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.175850, mae: 2.561127, mean_q: 5.282858\n",
      " 1041/5000: episode: 105, duration: 0.124s, episode steps:  12, steps per second:  97, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.165032, mae: 2.500011, mean_q: 5.139798\n",
      " 1051/5000: episode: 106, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.107715, mae: 2.537265, mean_q: 5.354078\n",
      " 1063/5000: episode: 107, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.148891, mae: 2.655008, mean_q: 5.566271\n",
      " 1073/5000: episode: 108, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.127077, mae: 2.562770, mean_q: 5.292666\n",
      " 1081/5000: episode: 109, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.175466, mae: 2.569158, mean_q: 5.214599\n",
      " 1091/5000: episode: 110, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.127842, mae: 2.677470, mean_q: 5.459952\n",
      " 1102/5000: episode: 111, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.139692, mae: 2.725059, mean_q: 5.488972\n",
      " 1115/5000: episode: 112, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.132763, mae: 2.730668, mean_q: 5.418899\n",
      " 1125/5000: episode: 113, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.163780, mae: 2.858262, mean_q: 5.678242\n",
      " 1135/5000: episode: 114, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.138094, mae: 2.778864, mean_q: 5.530292\n",
      " 1145/5000: episode: 115, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.152192, mae: 2.838042, mean_q: 5.663003\n",
      " 1155/5000: episode: 116, duration: 0.081s, episode steps:  10, steps per second: 124, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.146988, mae: 2.798489, mean_q: 5.535363\n",
      " 1170/5000: episode: 117, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.160786, mae: 2.860508, mean_q: 5.598755\n",
      " 1191/5000: episode: 118, duration: 0.166s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.151228, mae: 2.919166, mean_q: 5.665442\n",
      " 1215/5000: episode: 119, duration: 0.189s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.158067, mae: 3.004458, mean_q: 5.816862\n",
      " 1290/5000: episode: 120, duration: 0.487s, episode steps:  75, steps per second: 154, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.181953, mae: 3.124789, mean_q: 6.002997\n",
      " 1301/5000: episode: 121, duration: 0.081s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.428907, mae: 3.386678, mean_q: 6.544149\n",
      " 1311/5000: episode: 122, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.223875, mae: 3.384586, mean_q: 6.542876\n",
      " 1319/5000: episode: 123, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.689090, mae: 3.398683, mean_q: 6.416755\n",
      " 1330/5000: episode: 124, duration: 0.089s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.770974, mae: 3.471429, mean_q: 6.577984\n",
      " 1339/5000: episode: 125, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.879239, mae: 3.431638, mean_q: 6.534955\n",
      " 1347/5000: episode: 126, duration: 0.058s, episode steps:   8, steps per second: 137, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.564782, mae: 3.490898, mean_q: 6.626907\n",
      " 1359/5000: episode: 127, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.747269, mae: 3.432998, mean_q: 6.454874\n",
      " 1369/5000: episode: 128, duration: 0.073s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.858996, mae: 3.600417, mean_q: 6.791715\n",
      " 1379/5000: episode: 129, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.855213, mae: 3.642628, mean_q: 6.867444\n",
      " 1390/5000: episode: 130, duration: 0.106s, episode steps:  11, steps per second: 103, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.817466, mae: 3.652436, mean_q: 6.875018\n",
      " 1403/5000: episode: 131, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.526866, mae: 3.751100, mean_q: 7.026595\n",
      " 1413/5000: episode: 132, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.191458, mae: 3.779577, mean_q: 7.116010\n",
      " 1423/5000: episode: 133, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.493570, mae: 3.960593, mean_q: 7.455736\n",
      " 1432/5000: episode: 134, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.562995, mae: 3.966238, mean_q: 7.360623\n",
      " 1441/5000: episode: 135, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.014351, mae: 4.002041, mean_q: 7.543159\n",
      " 1451/5000: episode: 136, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.769453, mae: 3.873081, mean_q: 7.153304\n",
      " 1460/5000: episode: 137, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.609552, mae: 4.202172, mean_q: 7.796690\n",
      " 1470/5000: episode: 138, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.800133, mae: 4.207593, mean_q: 7.751671\n",
      " 1480/5000: episode: 139, duration: 0.084s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.714843, mae: 4.046964, mean_q: 7.400357\n",
      " 1488/5000: episode: 140, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.853396, mae: 4.234200, mean_q: 7.722662\n",
      " 1498/5000: episode: 141, duration: 0.075s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.860365, mae: 4.049325, mean_q: 7.603332\n",
      " 1506/5000: episode: 142, duration: 0.058s, episode steps:   8, steps per second: 138, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 1.360387, mae: 4.232149, mean_q: 7.910940\n",
      " 1519/5000: episode: 143, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.923 [0.000, 1.000],  loss: 1.253224, mae: 4.146603, mean_q: 7.759176\n",
      " 1530/5000: episode: 144, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.929489, mae: 4.341037, mean_q: 8.095734\n",
      " 1540/5000: episode: 145, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.109406, mae: 4.145661, mean_q: 7.813289\n",
      " 1550/5000: episode: 146, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.627602, mae: 4.616586, mean_q: 8.458358\n",
      " 1559/5000: episode: 147, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.701714, mae: 4.310450, mean_q: 8.099024\n",
      " 1570/5000: episode: 148, duration: 0.097s, episode steps:  11, steps per second: 113, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 2.743944, mae: 4.490588, mean_q: 8.153951\n",
      " 1578/5000: episode: 149, duration: 0.069s, episode steps:   8, steps per second: 117, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.728189, mae: 4.674646, mean_q: 8.506428\n",
      " 1587/5000: episode: 150, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.008925, mae: 4.383834, mean_q: 8.030649\n",
      " 1599/5000: episode: 151, duration: 0.084s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.065975, mae: 4.569411, mean_q: 8.287073\n",
      " 1628/5000: episode: 152, duration: 0.213s, episode steps:  29, steps per second: 136, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.146794, mae: 4.689915, mean_q: 8.476946\n",
      " 1639/5000: episode: 153, duration: 0.087s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 2.338665, mae: 4.617485, mean_q: 8.368224\n",
      " 1648/5000: episode: 154, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.777564, mae: 4.468992, mean_q: 8.138086\n",
      " 1658/5000: episode: 155, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.350867, mae: 4.575136, mean_q: 8.431157\n",
      " 1667/5000: episode: 156, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.196248, mae: 4.565466, mean_q: 8.492327\n",
      " 1679/5000: episode: 157, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 2.156525, mae: 4.661541, mean_q: 8.603080\n",
      " 1689/5000: episode: 158, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.548775, mae: 4.761931, mean_q: 8.560397\n",
      " 1700/5000: episode: 159, duration: 0.098s, episode steps:  11, steps per second: 112, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.383281, mae: 4.886190, mean_q: 8.735401\n",
      " 1709/5000: episode: 160, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.408041, mae: 4.787746, mean_q: 8.635372\n",
      " 1725/5000: episode: 161, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.434352, mae: 4.731735, mean_q: 8.698306\n",
      " 1740/5000: episode: 162, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.483907, mae: 4.765254, mean_q: 8.825713\n",
      " 1750/5000: episode: 163, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.538243, mae: 4.905709, mean_q: 9.105046\n",
      " 1761/5000: episode: 164, duration: 0.084s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.800436, mae: 4.914216, mean_q: 9.086091\n",
      " 1771/5000: episode: 165, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.636188, mae: 5.174566, mean_q: 9.422307\n",
      " 1784/5000: episode: 166, duration: 0.090s, episode steps:  13, steps per second: 145, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.892557, mae: 4.885582, mean_q: 8.994112\n",
      " 1795/5000: episode: 167, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.828315, mae: 5.006595, mean_q: 9.255615\n",
      " 1806/5000: episode: 168, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.508837, mae: 5.087502, mean_q: 9.270679\n",
      " 1815/5000: episode: 169, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.663846, mae: 5.063724, mean_q: 9.215888\n",
      " 1825/5000: episode: 170, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.285554, mae: 4.977001, mean_q: 9.056029\n",
      " 1837/5000: episode: 171, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.713232, mae: 5.226620, mean_q: 9.380569\n",
      " 1849/5000: episode: 172, duration: 0.084s, episode steps:  12, steps per second: 143, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.278391, mae: 5.188574, mean_q: 9.361944\n",
      " 1858/5000: episode: 173, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.139846, mae: 5.028038, mean_q: 9.104177\n",
      " 1872/5000: episode: 174, duration: 0.097s, episode steps:  14, steps per second: 144, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.332112, mae: 5.063263, mean_q: 9.205779\n",
      " 1885/5000: episode: 175, duration: 0.089s, episode steps:  13, steps per second: 146, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.815527, mae: 4.977741, mean_q: 9.117074\n",
      " 1896/5000: episode: 176, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.609996, mae: 4.919372, mean_q: 9.011959\n",
      " 1908/5000: episode: 177, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.359429, mae: 5.005377, mean_q: 9.304414\n",
      " 1923/5000: episode: 178, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.026473, mae: 5.071120, mean_q: 9.322224\n",
      " 1937/5000: episode: 179, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.783805, mae: 5.015954, mean_q: 9.328826\n",
      " 1946/5000: episode: 180, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.346063, mae: 5.179274, mean_q: 9.565866\n",
      " 1956/5000: episode: 181, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.173305, mae: 5.292766, mean_q: 9.626169\n",
      " 1967/5000: episode: 182, duration: 0.078s, episode steps:  11, steps per second: 140, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.089727, mae: 5.353709, mean_q: 9.803109\n",
      " 1979/5000: episode: 183, duration: 0.089s, episode steps:  12, steps per second: 134, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.992420, mae: 5.116322, mean_q: 9.355880\n",
      " 1988/5000: episode: 184, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.768651, mae: 5.361492, mean_q: 9.676370\n",
      " 1999/5000: episode: 185, duration: 0.081s, episode steps:  11, steps per second: 137, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.377684, mae: 5.093532, mean_q: 9.440238\n",
      " 2011/5000: episode: 186, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.456160, mae: 5.222158, mean_q: 9.525428\n",
      " 2020/5000: episode: 187, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 2.921097, mae: 5.426431, mean_q: 9.735689\n",
      " 2031/5000: episode: 188, duration: 0.079s, episode steps:  11, steps per second: 140, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.556019, mae: 5.291175, mean_q: 9.538412\n",
      " 2042/5000: episode: 189, duration: 0.081s, episode steps:  11, steps per second: 137, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.669502, mae: 5.402601, mean_q: 9.758667\n",
      " 2053/5000: episode: 190, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.789083, mae: 5.435702, mean_q: 9.708772\n",
      " 2067/5000: episode: 191, duration: 0.097s, episode steps:  14, steps per second: 145, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.063720, mae: 5.107166, mean_q: 9.286275\n",
      " 2076/5000: episode: 192, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 2.065312, mae: 5.256558, mean_q: 9.642045\n",
      " 2084/5000: episode: 193, duration: 0.059s, episode steps:   8, steps per second: 137, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 1.977739, mae: 5.215040, mean_q: 9.680275\n",
      " 2094/5000: episode: 194, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.033637, mae: 5.304952, mean_q: 9.827216\n",
      " 2103/5000: episode: 195, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 2.229242, mae: 5.330504, mean_q: 9.896465\n",
      " 2115/5000: episode: 196, duration: 0.092s, episode steps:  12, steps per second: 131, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.466022, mae: 5.245670, mean_q: 9.578727\n",
      " 2125/5000: episode: 197, duration: 0.073s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.021622, mae: 5.308745, mean_q: 9.764007\n",
      " 2135/5000: episode: 198, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.572939, mae: 5.198233, mean_q: 9.640387\n",
      " 2145/5000: episode: 199, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.868370, mae: 5.458135, mean_q: 10.006166\n",
      " 2154/5000: episode: 200, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.327552, mae: 5.359217, mean_q: 9.829094\n",
      " 2164/5000: episode: 201, duration: 0.071s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.793182, mae: 5.425431, mean_q: 9.793942\n",
      " 2174/5000: episode: 202, duration: 0.071s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.554957, mae: 5.344905, mean_q: 9.946471\n",
      " 2184/5000: episode: 203, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.770651, mae: 5.263101, mean_q: 9.719543\n",
      " 2194/5000: episode: 204, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.718866, mae: 5.274359, mean_q: 9.539238\n",
      " 2204/5000: episode: 205, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.318621, mae: 5.353002, mean_q: 9.789959\n",
      " 2214/5000: episode: 206, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.042439, mae: 5.325928, mean_q: 9.676736\n",
      " 2224/5000: episode: 207, duration: 0.064s, episode steps:  10, steps per second: 156, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.979992, mae: 5.123855, mean_q: 9.266483\n",
      " 2233/5000: episode: 208, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 2.001132, mae: 5.358795, mean_q: 9.789699\n",
      " 2246/5000: episode: 209, duration: 0.086s, episode steps:  13, steps per second: 151, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.035791, mae: 5.266985, mean_q: 9.583337\n",
      " 2258/5000: episode: 210, duration: 0.075s, episode steps:  12, steps per second: 160, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.139156, mae: 5.174687, mean_q: 9.627421\n",
      " 2268/5000: episode: 211, duration: 0.063s, episode steps:  10, steps per second: 158, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.063483, mae: 5.196030, mean_q: 9.520391\n",
      " 2280/5000: episode: 212, duration: 0.074s, episode steps:  12, steps per second: 161, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.221524, mae: 5.168800, mean_q: 9.644222\n",
      " 2291/5000: episode: 213, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.326782, mae: 5.255701, mean_q: 9.872870\n",
      " 2302/5000: episode: 214, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.311386, mae: 5.338636, mean_q: 9.752720\n",
      " 2312/5000: episode: 215, duration: 0.064s, episode steps:  10, steps per second: 157, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.147094, mae: 5.424300, mean_q: 9.897202\n",
      " 2324/5000: episode: 216, duration: 0.080s, episode steps:  12, steps per second: 150, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.147523, mae: 5.170386, mean_q: 9.601912\n",
      " 2335/5000: episode: 217, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.471116, mae: 5.171622, mean_q: 9.649220\n",
      " 2346/5000: episode: 218, duration: 0.069s, episode steps:  11, steps per second: 159, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.454085, mae: 5.141418, mean_q: 9.599136\n",
      " 2357/5000: episode: 219, duration: 0.072s, episode steps:  11, steps per second: 153, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.601834, mae: 5.289559, mean_q: 9.889360\n",
      " 2367/5000: episode: 220, duration: 0.067s, episode steps:  10, steps per second: 150, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.638186, mae: 5.193333, mean_q: 9.664929\n",
      " 2378/5000: episode: 221, duration: 0.070s, episode steps:  11, steps per second: 156, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.304392, mae: 5.170162, mean_q: 9.691031\n",
      " 2392/5000: episode: 222, duration: 0.091s, episode steps:  14, steps per second: 154, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.432624, mae: 5.170153, mean_q: 9.563794\n",
      " 2426/5000: episode: 223, duration: 0.206s, episode steps:  34, steps per second: 165, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 1.388741, mae: 5.225547, mean_q: 9.732803\n",
      " 2466/5000: episode: 224, duration: 0.245s, episode steps:  40, steps per second: 163, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.597215, mae: 5.291853, mean_q: 9.852536\n",
      " 2507/5000: episode: 225, duration: 0.255s, episode steps:  41, steps per second: 161, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 1.720013, mae: 5.212484, mean_q: 9.592545\n",
      " 2531/5000: episode: 226, duration: 0.144s, episode steps:  24, steps per second: 167, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.586235, mae: 5.188661, mean_q: 9.598935\n",
      " 2600/5000: episode: 227, duration: 0.411s, episode steps:  69, steps per second: 168, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 1.499184, mae: 5.244409, mean_q: 9.756021\n",
      " 2650/5000: episode: 228, duration: 0.305s, episode steps:  50, steps per second: 164, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.624344, mae: 5.332025, mean_q: 9.890148\n",
      " 2670/5000: episode: 229, duration: 0.125s, episode steps:  20, steps per second: 160, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.712711, mae: 5.348974, mean_q: 9.909319\n",
      " 2685/5000: episode: 230, duration: 0.097s, episode steps:  15, steps per second: 155, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.798831, mae: 5.396034, mean_q: 9.977740\n",
      " 2714/5000: episode: 231, duration: 0.177s, episode steps:  29, steps per second: 164, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 1.341622, mae: 5.341822, mean_q: 9.969690\n",
      " 2742/5000: episode: 232, duration: 0.172s, episode steps:  28, steps per second: 163, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.095221, mae: 5.281876, mean_q: 9.918320\n",
      " 2785/5000: episode: 233, duration: 0.329s, episode steps:  43, steps per second: 131, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.303417, mae: 5.322686, mean_q: 9.969263\n",
      " 2834/5000: episode: 234, duration: 0.350s, episode steps:  49, steps per second: 140, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.485976, mae: 5.452389, mean_q: 10.176077\n",
      " 2875/5000: episode: 235, duration: 0.296s, episode steps:  41, steps per second: 139, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.507582, mae: 5.435224, mean_q: 10.148046\n",
      " 2970/5000: episode: 236, duration: 0.679s, episode steps:  95, steps per second: 140, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.284192, mae: 5.582899, mean_q: 10.480599\n",
      " 3035/5000: episode: 237, duration: 0.437s, episode steps:  65, steps per second: 149, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.545588, mae: 5.709102, mean_q: 10.693393\n",
      " 3116/5000: episode: 238, duration: 0.562s, episode steps:  81, steps per second: 144, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.324664, mae: 5.868299, mean_q: 11.078139\n",
      " 3148/5000: episode: 239, duration: 0.245s, episode steps:  32, steps per second: 131, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.708042, mae: 5.890172, mean_q: 11.005904\n",
      " 3221/5000: episode: 240, duration: 0.537s, episode steps:  73, steps per second: 136, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.364676, mae: 5.970046, mean_q: 11.251729\n",
      " 3324/5000: episode: 241, duration: 0.695s, episode steps: 103, steps per second: 148, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.654124, mae: 6.067953, mean_q: 11.387177\n",
      " 3400/5000: episode: 242, duration: 0.514s, episode steps:  76, steps per second: 148, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.713789, mae: 6.289916, mean_q: 11.824903\n",
      " 3455/5000: episode: 243, duration: 0.371s, episode steps:  55, steps per second: 148, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.396040, mae: 6.268088, mean_q: 11.844200\n",
      " 3506/5000: episode: 244, duration: 0.348s, episode steps:  51, steps per second: 147, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.432458, mae: 6.330042, mean_q: 11.982047\n",
      " 3549/5000: episode: 245, duration: 0.297s, episode steps:  43, steps per second: 145, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.243902, mae: 6.459802, mean_q: 12.095635\n",
      " 3608/5000: episode: 246, duration: 0.397s, episode steps:  59, steps per second: 149, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.866338, mae: 6.495570, mean_q: 12.163169\n",
      " 3676/5000: episode: 247, duration: 0.469s, episode steps:  68, steps per second: 145, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.192757, mae: 6.621689, mean_q: 12.397739\n",
      " 3745/5000: episode: 248, duration: 0.467s, episode steps:  69, steps per second: 148, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.878985, mae: 6.651067, mean_q: 12.503267\n",
      " 3779/5000: episode: 249, duration: 0.232s, episode steps:  34, steps per second: 147, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.635389, mae: 6.740967, mean_q: 12.698789\n",
      " 3828/5000: episode: 250, duration: 0.332s, episode steps:  49, steps per second: 147, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.906020, mae: 6.802663, mean_q: 12.782917\n",
      " 3881/5000: episode: 251, duration: 0.409s, episode steps:  53, steps per second: 130, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.308978, mae: 6.866386, mean_q: 13.041054\n",
      " 3945/5000: episode: 252, duration: 0.515s, episode steps:  64, steps per second: 124, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.817657, mae: 7.009690, mean_q: 13.235720\n",
      " 3975/5000: episode: 253, duration: 0.189s, episode steps:  30, steps per second: 159, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.333175, mae: 7.062620, mean_q: 13.248689\n",
      " 4004/5000: episode: 254, duration: 0.197s, episode steps:  29, steps per second: 147, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.616064, mae: 7.171416, mean_q: 13.596741\n",
      " 4071/5000: episode: 255, duration: 0.471s, episode steps:  67, steps per second: 142, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.014258, mae: 7.161109, mean_q: 13.530764\n",
      " 4101/5000: episode: 256, duration: 0.183s, episode steps:  30, steps per second: 164, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.665231, mae: 7.290845, mean_q: 13.878770\n",
      " 4139/5000: episode: 257, duration: 0.234s, episode steps:  38, steps per second: 162, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.896305, mae: 7.353431, mean_q: 13.908330\n",
      " 4206/5000: episode: 258, duration: 0.412s, episode steps:  67, steps per second: 163, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.234310, mae: 7.505189, mean_q: 14.162551\n",
      " 4250/5000: episode: 259, duration: 0.319s, episode steps:  44, steps per second: 138, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.843694, mae: 7.545978, mean_q: 14.321593\n",
      " 4271/5000: episode: 260, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.872657, mae: 7.621416, mean_q: 14.459725\n",
      " 4353/5000: episode: 261, duration: 0.551s, episode steps:  82, steps per second: 149, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.523898, mae: 7.685760, mean_q: 14.480701\n",
      " 4480/5000: episode: 262, duration: 0.884s, episode steps: 127, steps per second: 144, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.121605, mae: 7.797561, mean_q: 14.732873\n",
      " 4544/5000: episode: 263, duration: 0.459s, episode steps:  64, steps per second: 140, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.871013, mae: 7.881719, mean_q: 14.966476\n",
      " 4612/5000: episode: 264, duration: 0.506s, episode steps:  68, steps per second: 134, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.563945, mae: 8.017033, mean_q: 15.111617\n",
      " 4642/5000: episode: 265, duration: 0.199s, episode steps:  30, steps per second: 151, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.183776, mae: 8.004295, mean_q: 15.172970\n",
      " 4701/5000: episode: 266, duration: 0.473s, episode steps:  59, steps per second: 125, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.312409, mae: 8.174069, mean_q: 15.455089\n",
      " 4741/5000: episode: 267, duration: 0.273s, episode steps:  40, steps per second: 146, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.441916, mae: 8.260109, mean_q: 15.590100\n",
      " 4786/5000: episode: 268, duration: 0.348s, episode steps:  45, steps per second: 129, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.396830, mae: 8.222147, mean_q: 15.507134\n",
      " 4857/5000: episode: 269, duration: 0.546s, episode steps:  71, steps per second: 130, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.294818, mae: 8.411397, mean_q: 15.950232\n",
      " 4920/5000: episode: 270, duration: 0.452s, episode steps:  63, steps per second: 139, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.153865, mae: 8.548246, mean_q: 16.259260\n",
      " 4962/5000: episode: 271, duration: 0.310s, episode steps:  42, steps per second: 136, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.906090, mae: 8.602253, mean_q: 16.300005\n",
      "done, took 36.275 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 76.000, steps: 76\n",
      "Episode 2: reward: 107.000, steps: 107\n",
      "Episode 3: reward: 31.000, steps: 31\n",
      "Episode 4: reward: 51.000, steps: 51\n",
      "Episode 5: reward: 46.000, steps: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b73805e850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)\n",
    "\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
